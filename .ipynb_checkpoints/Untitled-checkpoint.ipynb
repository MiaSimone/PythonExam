{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The directory exists: /home/jovyan/Exam_Project/Dataset/Banan\n",
      "The directory exists: /home/jovyan/Exam_Project/Dataset/Banan\n",
      "The directory exists: /home/jovyan/Exam_Project/Dataset/Banan\n",
      "The directory exists: /home/jovyan/Exam_Project/Dataset/Banan\n",
      "The directory exists: /home/jovyan/Exam_Project/Dataset/Banan\n",
      "The directory exists: /home/jovyan/Exam_Project/Dataset/Banan\n",
      "The directory exists: /home/jovyan/Exam_Project/Dataset/Banan\n",
      "The directory exists: /home/jovyan/Exam_Project/Dataset/Banan\n",
      "The directory exists: /home/jovyan/Exam_Project/Dataset/Banan\n",
      "The directory exists: /home/jovyan/Exam_Project/Dataset/Banan\n",
      "The directory exists: /home/jovyan/Exam_Project/Dataset/Banan\n",
      "The directory exists: /home/jovyan/Exam_Project/Dataset/Banan\n",
      "The directory exists: /home/jovyan/Exam_Project/Dataset/Banan\n",
      "The directory exists: /home/jovyan/Exam_Project/Dataset/Banan\n",
      "The directory exists: /home/jovyan/Exam_Project/Dataset/Banan\n",
      "The directory exists: /home/jovyan/Exam_Project/Dataset/Banan\n",
      "The directory exists: /home/jovyan/Exam_Project/Dataset/Banan\n",
      "The directory exists: /home/jovyan/Exam_Project/Dataset/Banan\n",
      "The directory exists: /home/jovyan/Exam_Project/Dataset/Banan\n",
      "Resizing images in Banan folder.\n",
      "Loading...\n",
      "The directory exists: /home/jovyan/Exam_Project/Dataset/Æble\n",
      "The directory exists: /home/jovyan/Exam_Project/Dataset/Æble\n",
      "The directory exists: /home/jovyan/Exam_Project/Dataset/Æble\n",
      "The directory exists: /home/jovyan/Exam_Project/Dataset/Æble\n",
      "The directory exists: /home/jovyan/Exam_Project/Dataset/Æble\n",
      "The directory exists: /home/jovyan/Exam_Project/Dataset/Æble\n",
      "The directory exists: /home/jovyan/Exam_Project/Dataset/Æble\n",
      "The directory exists: /home/jovyan/Exam_Project/Dataset/Æble\n",
      "The directory exists: /home/jovyan/Exam_Project/Dataset/Æble\n",
      "The directory exists: /home/jovyan/Exam_Project/Dataset/Æble\n",
      "The directory exists: /home/jovyan/Exam_Project/Dataset/Æble\n",
      "The directory exists: /home/jovyan/Exam_Project/Dataset/Æble\n",
      "The directory exists: /home/jovyan/Exam_Project/Dataset/Æble\n",
      "The directory exists: /home/jovyan/Exam_Project/Dataset/Æble\n",
      "The directory exists: /home/jovyan/Exam_Project/Dataset/Æble\n",
      "The directory exists: /home/jovyan/Exam_Project/Dataset/Æble\n",
      "The directory exists: /home/jovyan/Exam_Project/Dataset/Æble\n",
      "The directory exists: /home/jovyan/Exam_Project/Dataset/Æble\n",
      "The directory exists: /home/jovyan/Exam_Project/Dataset/Æble\n",
      "Resizing images in Æble folder.\n",
      "Loading...\n",
      "The directory exists: /home/jovyan/Exam_Project/Dataset/Appelsin\n",
      "The directory exists: /home/jovyan/Exam_Project/Dataset/Appelsin\n",
      "The directory exists: /home/jovyan/Exam_Project/Dataset/Appelsin\n",
      "The directory exists: /home/jovyan/Exam_Project/Dataset/Appelsin\n",
      "The directory exists: /home/jovyan/Exam_Project/Dataset/Appelsin\n",
      "The directory exists: /home/jovyan/Exam_Project/Dataset/Appelsin\n",
      "The directory exists: /home/jovyan/Exam_Project/Dataset/Appelsin\n",
      "The directory exists: /home/jovyan/Exam_Project/Dataset/Appelsin\n",
      "The directory exists: /home/jovyan/Exam_Project/Dataset/Appelsin\n",
      "The directory exists: /home/jovyan/Exam_Project/Dataset/Appelsin\n",
      "The directory exists: /home/jovyan/Exam_Project/Dataset/Appelsin\n",
      "The directory exists: /home/jovyan/Exam_Project/Dataset/Appelsin\n",
      "The directory exists: /home/jovyan/Exam_Project/Dataset/Appelsin\n",
      "The directory exists: /home/jovyan/Exam_Project/Dataset/Appelsin\n",
      "The directory exists: /home/jovyan/Exam_Project/Dataset/Appelsin\n",
      "The directory exists: /home/jovyan/Exam_Project/Dataset/Appelsin\n",
      "The directory exists: /home/jovyan/Exam_Project/Dataset/Appelsin\n",
      "The directory exists: /home/jovyan/Exam_Project/Dataset/Appelsin\n",
      "The directory exists: /home/jovyan/Exam_Project/Dataset/Appelsin\n",
      "Resizing images in Appelsin folder.\n",
      "Loading...\n",
      "The directory exists: /home/jovyan/Exam_Project/Dataset/Pære\n",
      "The directory exists: /home/jovyan/Exam_Project/Dataset/Pære\n",
      "The directory exists: /home/jovyan/Exam_Project/Dataset/Pære\n",
      "The directory exists: /home/jovyan/Exam_Project/Dataset/Pære\n",
      "The directory exists: /home/jovyan/Exam_Project/Dataset/Pære\n",
      "The directory exists: /home/jovyan/Exam_Project/Dataset/Pære\n",
      "The directory exists: /home/jovyan/Exam_Project/Dataset/Pære\n",
      "The directory exists: /home/jovyan/Exam_Project/Dataset/Pære\n",
      "The directory exists: /home/jovyan/Exam_Project/Dataset/Pære\n",
      "The directory exists: /home/jovyan/Exam_Project/Dataset/Pære\n",
      "The directory exists: /home/jovyan/Exam_Project/Dataset/Pære\n",
      "The directory exists: /home/jovyan/Exam_Project/Dataset/Pære\n",
      "The directory exists: /home/jovyan/Exam_Project/Dataset/Pære\n",
      "The directory exists: /home/jovyan/Exam_Project/Dataset/Pære\n",
      "The directory exists: /home/jovyan/Exam_Project/Dataset/Pære\n",
      "The directory exists: /home/jovyan/Exam_Project/Dataset/Pære\n",
      "The directory exists: /home/jovyan/Exam_Project/Dataset/Pære\n",
      "The directory exists: /home/jovyan/Exam_Project/Dataset/Pære\n",
      "The directory exists: /home/jovyan/Exam_Project/Dataset/Pære\n",
      "Resizing images in Pære folder.\n",
      "Loading...\n",
      "The directory exists: /home/jovyan/Exam_Project/Dataset/Kiwi\n",
      "The directory exists: /home/jovyan/Exam_Project/Dataset/Kiwi\n",
      "The directory exists: /home/jovyan/Exam_Project/Dataset/Kiwi\n",
      "The directory exists: /home/jovyan/Exam_Project/Dataset/Kiwi\n",
      "The directory exists: /home/jovyan/Exam_Project/Dataset/Kiwi\n",
      "The directory exists: /home/jovyan/Exam_Project/Dataset/Kiwi\n",
      "The directory exists: /home/jovyan/Exam_Project/Dataset/Kiwi\n",
      "The directory exists: /home/jovyan/Exam_Project/Dataset/Kiwi\n",
      "The directory exists: /home/jovyan/Exam_Project/Dataset/Kiwi\n",
      "The directory exists: /home/jovyan/Exam_Project/Dataset/Kiwi\n",
      "The directory exists: /home/jovyan/Exam_Project/Dataset/Kiwi\n",
      "The directory exists: /home/jovyan/Exam_Project/Dataset/Kiwi\n",
      "The directory exists: /home/jovyan/Exam_Project/Dataset/Kiwi\n",
      "The directory exists: /home/jovyan/Exam_Project/Dataset/Kiwi\n",
      "The directory exists: /home/jovyan/Exam_Project/Dataset/Kiwi\n",
      "The directory exists: /home/jovyan/Exam_Project/Dataset/Kiwi\n",
      "The directory exists: /home/jovyan/Exam_Project/Dataset/Kiwi\n",
      "The directory exists: /home/jovyan/Exam_Project/Dataset/Kiwi\n",
      "The directory exists: /home/jovyan/Exam_Project/Dataset/Kiwi\n",
      "Resizing images in Kiwi folder.\n",
      "Loading...\n",
      "The directory exists: /home/jovyan/Exam_Project/Dataset/Citron\n",
      "The directory exists: /home/jovyan/Exam_Project/Dataset/Citron\n",
      "The directory exists: /home/jovyan/Exam_Project/Dataset/Citron\n",
      "The directory exists: /home/jovyan/Exam_Project/Dataset/Citron\n",
      "The directory exists: /home/jovyan/Exam_Project/Dataset/Citron\n",
      "The directory exists: /home/jovyan/Exam_Project/Dataset/Citron\n",
      "The directory exists: /home/jovyan/Exam_Project/Dataset/Citron\n",
      "The directory exists: /home/jovyan/Exam_Project/Dataset/Citron\n",
      "The directory exists: /home/jovyan/Exam_Project/Dataset/Citron\n",
      "The directory exists: /home/jovyan/Exam_Project/Dataset/Citron\n",
      "The directory exists: /home/jovyan/Exam_Project/Dataset/Citron\n",
      "The directory exists: /home/jovyan/Exam_Project/Dataset/Citron\n",
      "The directory exists: /home/jovyan/Exam_Project/Dataset/Citron\n",
      "The directory exists: /home/jovyan/Exam_Project/Dataset/Citron\n",
      "The directory exists: /home/jovyan/Exam_Project/Dataset/Citron\n",
      "The directory exists: /home/jovyan/Exam_Project/Dataset/Citron\n",
      "The directory exists: /home/jovyan/Exam_Project/Dataset/Citron\n",
      "The directory exists: /home/jovyan/Exam_Project/Dataset/Citron\n",
      "The directory exists: /home/jovyan/Exam_Project/Dataset/Citron\n",
      "Resizing images in Citron folder.\n",
      "Loading...\n",
      "The directory exists: /home/jovyan/Exam_Project/Dataset/Vandmelon\n",
      "The directory exists: /home/jovyan/Exam_Project/Dataset/Vandmelon\n",
      "The directory exists: /home/jovyan/Exam_Project/Dataset/Vandmelon\n",
      "The directory exists: /home/jovyan/Exam_Project/Dataset/Vandmelon\n",
      "The directory exists: /home/jovyan/Exam_Project/Dataset/Vandmelon\n",
      "The directory exists: /home/jovyan/Exam_Project/Dataset/Vandmelon\n",
      "The directory exists: /home/jovyan/Exam_Project/Dataset/Vandmelon\n",
      "The directory exists: /home/jovyan/Exam_Project/Dataset/Vandmelon\n",
      "The directory exists: /home/jovyan/Exam_Project/Dataset/Vandmelon\n",
      "The directory exists: /home/jovyan/Exam_Project/Dataset/Vandmelon\n",
      "The directory exists: /home/jovyan/Exam_Project/Dataset/Vandmelon\n",
      "The directory exists: /home/jovyan/Exam_Project/Dataset/Vandmelon\n",
      "The directory exists: /home/jovyan/Exam_Project/Dataset/Vandmelon\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The directory exists: /home/jovyan/Exam_Project/Dataset/Vandmelon\n",
      "The directory exists: /home/jovyan/Exam_Project/Dataset/Vandmelon\n",
      "The directory exists: /home/jovyan/Exam_Project/Dataset/Vandmelon\n",
      "The directory exists: /home/jovyan/Exam_Project/Dataset/Vandmelon\n",
      "The directory exists: /home/jovyan/Exam_Project/Dataset/Vandmelon\n",
      "The directory exists: /home/jovyan/Exam_Project/Dataset/Vandmelon\n",
      "Resizing images in Vandmelon folder.\n",
      "Loading...\n",
      "The directory exists: /home/jovyan/Exam_Project/Dataset/Ananas\n",
      "The directory exists: /home/jovyan/Exam_Project/Dataset/Ananas\n",
      "The directory exists: /home/jovyan/Exam_Project/Dataset/Ananas\n",
      "The directory exists: /home/jovyan/Exam_Project/Dataset/Ananas\n",
      "The directory exists: /home/jovyan/Exam_Project/Dataset/Ananas\n",
      "The directory exists: /home/jovyan/Exam_Project/Dataset/Ananas\n",
      "The directory exists: /home/jovyan/Exam_Project/Dataset/Ananas\n",
      "The directory exists: /home/jovyan/Exam_Project/Dataset/Ananas\n",
      "The directory exists: /home/jovyan/Exam_Project/Dataset/Ananas\n",
      "The directory exists: /home/jovyan/Exam_Project/Dataset/Ananas\n",
      "The directory exists: /home/jovyan/Exam_Project/Dataset/Ananas\n",
      "The directory exists: /home/jovyan/Exam_Project/Dataset/Ananas\n",
      "The directory exists: /home/jovyan/Exam_Project/Dataset/Ananas\n",
      "The directory exists: /home/jovyan/Exam_Project/Dataset/Ananas\n",
      "The directory exists: /home/jovyan/Exam_Project/Dataset/Ananas\n",
      "The directory exists: /home/jovyan/Exam_Project/Dataset/Ananas\n",
      "The directory exists: /home/jovyan/Exam_Project/Dataset/Ananas\n",
      "The directory exists: /home/jovyan/Exam_Project/Dataset/Ananas\n",
      "The directory exists: /home/jovyan/Exam_Project/Dataset/Ananas\n",
      "Resizing images in Ananas folder.\n",
      "Loading...\n",
      "The directory exists: /home/jovyan/Exam_Project/Dataset/Vindruer\n",
      "The directory exists: /home/jovyan/Exam_Project/Dataset/Vindruer\n",
      "The directory exists: /home/jovyan/Exam_Project/Dataset/Vindruer\n",
      "The directory exists: /home/jovyan/Exam_Project/Dataset/Vindruer\n",
      "The directory exists: /home/jovyan/Exam_Project/Dataset/Vindruer\n",
      "The directory exists: /home/jovyan/Exam_Project/Dataset/Vindruer\n",
      "The directory exists: /home/jovyan/Exam_Project/Dataset/Vindruer\n",
      "The directory exists: /home/jovyan/Exam_Project/Dataset/Vindruer\n",
      "The directory exists: /home/jovyan/Exam_Project/Dataset/Vindruer\n",
      "The directory exists: /home/jovyan/Exam_Project/Dataset/Vindruer\n",
      "The directory exists: /home/jovyan/Exam_Project/Dataset/Vindruer\n",
      "The directory exists: /home/jovyan/Exam_Project/Dataset/Vindruer\n",
      "The directory exists: /home/jovyan/Exam_Project/Dataset/Vindruer\n",
      "The directory exists: /home/jovyan/Exam_Project/Dataset/Vindruer\n",
      "The directory exists: /home/jovyan/Exam_Project/Dataset/Vindruer\n",
      "The directory exists: /home/jovyan/Exam_Project/Dataset/Vindruer\n",
      "The directory exists: /home/jovyan/Exam_Project/Dataset/Vindruer\n",
      "The directory exists: /home/jovyan/Exam_Project/Dataset/Vindruer\n",
      "The directory exists: /home/jovyan/Exam_Project/Dataset/Vindruer\n",
      "Resizing images in Vindruer folder.\n",
      "Loading...\n",
      "The directory exists: /home/jovyan/Exam_Project/Dataset/Mango\n",
      "The directory exists: /home/jovyan/Exam_Project/Dataset/Mango\n",
      "The directory exists: /home/jovyan/Exam_Project/Dataset/Mango\n",
      "The directory exists: /home/jovyan/Exam_Project/Dataset/Mango\n",
      "The directory exists: /home/jovyan/Exam_Project/Dataset/Mango\n",
      "The directory exists: /home/jovyan/Exam_Project/Dataset/Mango\n",
      "The directory exists: /home/jovyan/Exam_Project/Dataset/Mango\n",
      "The directory exists: /home/jovyan/Exam_Project/Dataset/Mango\n",
      "The directory exists: /home/jovyan/Exam_Project/Dataset/Mango\n",
      "The directory exists: /home/jovyan/Exam_Project/Dataset/Mango\n",
      "The directory exists: /home/jovyan/Exam_Project/Dataset/Mango\n",
      "The directory exists: /home/jovyan/Exam_Project/Dataset/Mango\n",
      "The directory exists: /home/jovyan/Exam_Project/Dataset/Mango\n",
      "The directory exists: /home/jovyan/Exam_Project/Dataset/Mango\n",
      "The directory exists: /home/jovyan/Exam_Project/Dataset/Mango\n",
      "The directory exists: /home/jovyan/Exam_Project/Dataset/Mango\n",
      "The directory exists: /home/jovyan/Exam_Project/Dataset/Mango\n",
      "The directory exists: /home/jovyan/Exam_Project/Dataset/Mango\n",
      "The directory exists: /home/jovyan/Exam_Project/Dataset/Mango\n",
      "Resizing images in Mango folder.\n",
      "Dataset created!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pathlib\n",
    "import bs4\n",
    "import requests\n",
    "import PIL\n",
    "from PIL import Image\n",
    "\n",
    "def insert_data(label, number, url):\n",
    "    response = requests.get(url)\n",
    "  \n",
    "    # Parent Directory path\n",
    "    parent_dir = \"/home/jovyan/Exam_Project/Dataset\"\n",
    "    \n",
    "    path = os.path.join(parent_dir, label)\n",
    "    \n",
    "    \n",
    "  # Creating the directory based on the label:\n",
    "    file = pathlib.Path(path)\n",
    "    if file.exists ():\n",
    "        print(\"The directory exists: \" + str(path))\n",
    "    else:\n",
    "        os.mkdir(path)\n",
    "        \n",
    "    file = open(path+\"/\"+str(number)+\".png\", \"wb\")\n",
    "    file.write(response.content)\n",
    "    file.close()\n",
    "\n",
    "    \n",
    "def create_dataset():\n",
    "    \n",
    "    full_url_dict = {}\n",
    "    \n",
    "    full_url_dict[\"Banan\"] = \"https://www.google.com/search?q=banan&tbm=isch&ved=2ahUKEwj7-pvw2sPwAhXGySoKHbKZDsoQ2-cCegQIABAA&oq=banan&gs_lcp=CgNpbWcQAzIECCMQJzIECCMQJzIFCAAQsQMyAggAMgIIADICCAAyAggAMgIIADICCAAyAggAOgQIABBDOgcIABCxAxBDUNR-WMiBAWDagwFoAHAAeACAAaQBiAH_BJIBAzAuNZgBAKABAaoBC2d3cy13aXotaW1nwAEB&sclient=img&ei=vY-bYLuoBcaTqwGys7rQDA&bih=642&biw=1422&safe=off&hl=da\"\n",
    "    full_url_dict[\"Æble\"] = \"https://www.google.com/search?q=%C3%A6ble&safe=off&hl=da&sxsrf=ALeKk01rul_oObd0LP6-d3_Pikb2jbJAsQ:1620807611582&source=lnms&tbm=isch&sa=X&ved=2ahUKEwidvb_v2sPwAhXFpYsKHT--Ai4Q_AUoAXoECAEQAw&biw=1422&bih=642\"\n",
    "    full_url_dict[\"Appelsin\"] = \"https://www.google.com/search?q=appelsin&tbm=isch&ved=2ahUKEwiSgtP42sPwAhURuCoKHSZUAKYQ2-cCegQIABAA&oq=appelsin&gs_lcp=CgNpbWcQAzICCAAyAggAMgIIADICCAAyAggAMgIIADICCAAyAggAMgIIADICCAA6BAgjECc6BAgAEEM6BwgAELEDEEM6BQgAELEDUL7CAVjvyAFggssBaABwAHgAgAGHAYgB3AeSAQMwLjiYAQCgAQGqAQtnd3Mtd2l6LWltZ8ABAQ&sclient=img&ei=zo-bYJLjLpHwqgGmqIGwCg&bih=642&biw=1422&safe=off&hl=da\"\n",
    "    full_url_dict[\"Pære\"] = \"https://www.google.com/search?q=p%C3%A6re&tbm=isch&hl=da&safe=off&chips=q:p%C3%A6re,online_chips:frugt:5KT6Pd18OxY%3D&sa=X&ved=2ahUKEwjoq5iM28PwAhXs_CoKHUAsBBcQ4lYoAHoECAEQGg&biw=1404&bih=642\"\n",
    "    full_url_dict[\"Kiwi\"] = \"https://www.google.com/search?q=kiwi&tbm=isch&hl=da&safe=off&chips=q:kiwi,online_chips:fruit:uqEC2fb1yHw%3D&sa=X&ved=2ahUKEwibkbeU28PwAhULyCoKHUPVCEAQ4lYoAXoECAEQHA&biw=1404&bih=642\"\n",
    "    full_url_dict[\"Citron\"] = \"https://www.google.com/search?q=citron&tbm=isch&ved=2ahUKEwjg1e-d28PwAhVEtSoKHRbHCc8Q2-cCegQIABAA&oq=citron&gs_lcp=CgNpbWcQAzIHCAAQsQMQQzICCAAyAggAMgIIADICCAAyAggAMgIIADIECAAQQzICCAAyAggAOgQIIxAnOgUIABCxA1DNWViJa2CDbWgDcAB4AIABiwGIAcEGkgEDMS42mAEAoAEBqgELZ3dzLXdpei1pbWfAAQE&sclient=img&ei=HJCbYODXMsTqqgGWjqf4DA&bih=642&biw=1404&safe=off&hl=da\"\n",
    "    full_url_dict[\"Vandmelon\"] = \"https://www.google.com/search?q=vandmelon&tbm=isch&ved=2ahUKEwjewo2l28PwAhVOzyoKHSx8AU4Q2-cCegQIABAA&oq=vandmelon&gs_lcp=CgNpbWcQAzICCAAyAggAMgIIADICCAAyAggAMgIIADICCAAyAggAMgIIADICCAA6BAgjECc6BQgAELEDOgQIABBDOggIABCxAxCDAVD3TViOVmDzV2gAcAB4AIABlAGIAbgIkgEDMS44mAEAoAEBqgELZ3dzLXdpei1pbWfAAQE&sclient=img&ei=K5CbYJ6BPc6eqwGs-IXwBA&bih=642&biw=1404&safe=off&hl=da\"\n",
    "    full_url_dict[\"Ananas\"] = \"https://www.google.com/search?q=ananas&tbm=isch&ved=2ahUKEwiKx4Sr28PwAhXCpYsKHZO5BQUQ2-cCegQIABAA&oq=ananas&gs_lcp=CgNpbWcQAzIFCAAQsQMyAggAMgIIADICCAAyAggAMgIIADICCAAyAggAMgIIADICCAA6BAgjECc6BAgAEEM6BwgAELEDEEM6CAgAELEDEIMBUMN7WL6BAWC3ggFoAHAAeACAAYgBiAGhBJIBAzIuM5gBAKABAaoBC2d3cy13aXotaW1nwAEB&sclient=img&ei=OJCbYIrLGsLLrgST85Yo&bih=642&biw=1404&safe=off&hl=da\"\n",
    "    full_url_dict[\"Vindruer\"] = \"https://www.google.com/search?q=vindruer&tbm=isch&ved=2ahUKEwjg7vbF28PwAhXEtCoKHZYkA7kQ2-cCegQIABAA&oq=vindruer&gs_lcp=CgNpbWcQAzICCAAyAggAMgIIADICCAAyAggAMgIIADICCAAyAggAMgIIADICCAA6BAgjECc6BQgAELEDOgQIABBDUNGRAViMmQFg75kBaABwAHgBgAH3AYgBxgeSAQUxLjUuMZgBAKABAaoBC2d3cy13aXotaW1nwAEB&sclient=img&ei=cJCbYOD2MsTpqgGWyYzICw&bih=642&biw=1404&safe=off&hl=da\"\n",
    "    full_url_dict[\"Mango\"] = \"https://www.google.com/search?q=mango+fruit&tbm=isch&ved=2ahUKEwiugbTX28PwAhVFuCoKHVfGCkkQ2-cCegQIABAA&oq=mango+fruit&gs_lcp=CgNpbWcQAzICCAAyAggAMgQIABAeMgQIABAeMgQIABAeMgQIABAeMgQIABAeMgQIABAeMgQIABAeMgQIABAeOgQIABBDUPxJWOVbYIVdaABwAHgAgAGLAYgB4QWSAQMwLjaYAQCgAQGqAQtnd3Mtd2l6LWltZ8ABAQ&sclient=img&ei=lZCbYO7iHcXwqgHXjKvIBA&bih=642&biw=1404&safe=off&hl=da\"\n",
    "    \n",
    "#     for i in full_url_dict:\n",
    "#         print (i + \": \" + full_url_dict[i]);\n",
    "        \n",
    "    for i in full_url_dict:\n",
    "        r = requests.get(full_url_dict[i])\n",
    "        r.raise_for_status()\n",
    "        soup = bs4.BeautifulSoup(r.text, 'html.parser')\n",
    "    \n",
    "        src_list = []\n",
    "\n",
    "        for link in soup.find_all('img'):\n",
    "            src_list.append(link.get('src'))\n",
    "        src_list.pop(0)\n",
    "        \n",
    "        for index in range(len(src_list)):\n",
    "            insert_data(i, index, src_list[index])\n",
    "        \n",
    "        resize_images(i)\n",
    "        if (i==\"Mango\"):\n",
    "            print(\"Dataset created!\")\n",
    "        else:\n",
    "            print(\"Loading...\")\n",
    "    \n",
    "\n",
    "def resize_images(label):\n",
    "    print(\"Resizing images in \" + label + \" folder.\")\n",
    "    f = r'/home/jovyan/Exam_Project/Dataset/'+label\n",
    "    for file in os.listdir(f):\n",
    "        f_img = f+\"/\"+file\n",
    "        img = Image.open(f_img)\n",
    "        img = img.resize((100,100))\n",
    "        img.save(f_img)\n",
    "\n",
    "        \n",
    "create_dataset();     \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# https://data-flair.training/blogs/train-test-set-in-python-ml/\n",
    "# https://se.mathworks.com/discovery/image-recognition-matlab.html\n",
    "# https://www.analyticsvidhya.com/blog/2020/10/create-image-classification-model-python-keras/\n",
    "# https://stackabuse.com/image-recognition-in-python-with-tensorflow-and-keras/\n",
    "# https://www.geeksforgeeks.org/python-image-classification-using-keras/\n",
    "    \n",
    "# https://towardsdatascience.com/all-the-steps-to-build-your-first-image-classifier-with-code-cf244b015799"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80\n"
     ]
    }
   ],
   "source": [
    "\n",
    "banan1 = \"https://www.google.com/search?q=banana+pictures&hl=da&sxsrf=ALeKk007VOtS73ndx3OTlxHzdS2XFopC5Q:1621422368200&source=lnms&tbm=isch&sa=X&ved=2ahUKEwjs6KWCzdXwAhUyx4sKHXPJBagQ_AUoAXoECAEQAw&cshid=1621422450404255&biw=1422&bih=642\"\n",
    "banan2 = \"https://www.google.com/search?q=banan&tbm=isch&ved=2ahUKEwjJkeKXztXwAhUIvCoKHVDQCG0Q2-cCegQIABAA&oq=banan&gs_lcp=CgNpbWcQAzIECCMQJzIECCMQJzIFCAAQsQMyBQgAELEDMgIIADICCAAyBQgAELEDMgIIADIFCAAQsQMyAggAUJSHA1iUhwNgtokDaABwAHgAgAG1AYgBtQGSAQMwLjGYAQCgAQGqAQtnd3Mtd2l6LWltZ8ABAQ&sclient=img&ei=WfKkYImBKIj4qgHQoKPoBg&bih=642&biw=1422&hl=da\"\n",
    "banan3 = \"https://www.google.com/search?q=banana&tbm=isch&hl=da&chips=q:banana,online_chips:fruit:Bsn3PjGolqY%3D&sa=X&ved=2ahUKEwiws9C-ztXwAhUElIsKHZY4Cj8Q4lYoAHoECAEQGg&biw=1404&bih=642\"\n",
    "banan4 = \"https://www.google.com/search?q=banan&tbm=isch&hl=da&chips=q:banan,online_chips:spise+bananer:8dQCo8-IjD8%3D,online_chips:frugter:cT2U5S-7vew%3D&sa=X&ved=2ahUKEwiqhLyXz9XwAhUTvCoKHSARCokQ4lYoC3oECAEQMg&biw=1404&bih=642\"\n",
    "\n",
    "æble1 = \"\"\n",
    "æble2 = \"\"\n",
    "æble3 = \"\"\n",
    "æble4 = \"\"\n",
    "\n",
    "banan_link_list = [banan1, banan2, banan3, banan4]\n",
    "\n",
    "src_list = []\n",
    "\n",
    "for l in banan_link_list:\n",
    "    r = requests.get(l)\n",
    "    r.raise_for_status()\n",
    "    soup = bs4.BeautifulSoup(r.text, 'html.parser')\n",
    "    \n",
    "    for link in soup.find_all('img'):\n",
    "        src_list.append(link.get('src'))\n",
    "    src_list.pop(0)\n",
    "\n",
    "# r = requests.get(\"https://www.google.com/search?q=banana+pictures&hl=da&sxsrf=ALeKk007VOtS73ndx3OTlxHzdS2XFopC5Q:1621422368200&source=lnms&tbm=isch&sa=X&ved=2ahUKEwjs6KWCzdXwAhUyx4sKHXPJBagQ_AUoAXoECAEQAw&cshid=1621422450404255&biw=1422&bih=642\")\n",
    "# r.raise_for_status()\n",
    "# soup = bs4.BeautifulSoup(r.text, 'html.parser')\n",
    "# src_list = []\n",
    "\n",
    "# for link in soup.find_all('img'):\n",
    "#     src_list.append(link.get('src'))\n",
    "# src_list.pop(0)\n",
    "# for link in soup.find_all('img'):\n",
    "#     print(link.get('srcset'))\n",
    "print(len(src_list))\n",
    "\n",
    "# for a in soup.find_all('a', href=True):\n",
    "#     print(\"Found the URL:\"+ a['href'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reformat input\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "print(x_train.shape)\n",
    "x_train = x_train.reshape(x_train.shape[0],28,28,1)\n",
    "print(x_train.shape)\n",
    "x_train = x_train.astype('float32')\n",
    "x_train /= 255\n",
    "\n",
    "\n",
    "x_test = x_test.reshape(x_test.shape[0],28,28,1)\n",
    "x_test = x_test.astype('float32')\n",
    "x_test /= 255\n",
    "\n",
    "#reformat output\n",
    "y_train = to_categorical(y_train, num_classes)\n",
    "y_test = to_categorical(y_test, num_classes)\n",
    "\n",
    "from keras import models\n",
    "from keras import layers\n",
    "model2 = models.Sequential()\n",
    "# keras.layers.Conv2D(filters, kernel_size... filter is how many filters (windows of sub pixel set) kernel is the window size eg: 3x3 pixels\n",
    "model2.add(layers.Conv2D(32, (3,3), activation='relu',input_shape=(28,28,1)))\n",
    "model2.add(layers.MaxPool2D((2,2))) # Max Pooling to reduce the spatial dimensions of the output volume. pool_size: integer or tuple of 2 integers, factors by which to downscale (vertical, horizontal)\n",
    "model2.add(layers.Conv2D(64,(3,3),activation='relu')) # does not need input_shape, since it gets it from previous layer\n",
    "model2.add(layers.MaxPool2D((2,2)))\n",
    "model2.add(layers.Conv2D(64,(3,3),activation='relu'))\n",
    "model2.add(layers.Flatten()) # rewrite tensor to single vector of values\n",
    "model2.add(layers.Dense(64, activation='relu'))\n",
    "model2.add(layers.Dense(10, activation='softmax')) # softmax is good for output layer because Softmax outputs probabilities range. The range will 0 to 1, and the sum of all the probabilities will be equal to one. If the softmax function used for multi-classification model it returns the probabilities of each class and the target class will have the high probability.\n",
    "\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[255 255 255 ... 255 255 255]\n",
      " [255 255 255 ... 255 255 255]\n",
      " [255 255 255 ... 255 255 255]\n",
      " ...\n",
      " [255 255 255 ... 255 255 255]\n",
      " [255 255 255 ... 255 255 255]\n",
      " [255 255 255 ... 255 255 255]]\n",
      "Creating training data...\n",
      "200\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "import cv2\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "DATADIR = \"/home/jovyan/Exam_Project/Dataset\"\n",
    "\n",
    "# All the categories you want your neural network to detect\n",
    "CATEGORIES = [\"Banan\", \"Æble\", \"Appelsin\", \"Pære\", \"Kiwi\", \"Citron\", \"Vandmelon\", \"Ananas\", \"Vindruer\", \"Mango\"]\n",
    "\n",
    "# The size of the images that your neural network will use\n",
    "IMG_SIZE = 50\n",
    "\n",
    "# Checking or all images in the data folder\n",
    "for category in CATEGORIES :\n",
    "    path = os.path.join(DATADIR, category)\n",
    "    for img in os.listdir(path):\n",
    "        img_array = cv2.imread(os.path.join(path, img), cv2.IMREAD_GRAYSCALE)\n",
    "        \n",
    "print(img_array)\n",
    "\n",
    "training_data = []\n",
    "\n",
    "\n",
    "def create_training_data():\n",
    "    print(\"Creating training data...\")\n",
    "    for category in CATEGORIES :\n",
    "        path = os.path.join(DATADIR, category)\n",
    "        class_num = CATEGORIES.index(category)\n",
    "        for img in os.listdir(path):\n",
    "            try :\n",
    "                img_array = cv2.imread(os.path.join(path, img), cv2.IMREAD_GRAYSCALE)\n",
    "                new_array = cv2.resize(img_array, (IMG_SIZE, IMG_SIZE))\n",
    "                training_data.append([new_array, class_num])\n",
    "            except Exception as e:\n",
    "                pass\n",
    "            \n",
    "            \n",
    "create_training_data()\n",
    "print(len(training_data))\n",
    "random.shuffle(training_data)\n",
    "\n",
    "\n",
    "X = [] #features\n",
    "y = [] #labels\n",
    "\n",
    "for features, label in training_data:\n",
    "    X.append(features)\n",
    "    y.append(label)\n",
    "    \n",
    "X = np.array(X).reshape(-1, IMG_SIZE, IMG_SIZE, 1)\n",
    "\n",
    "# Creating the files containing all the information about your model\n",
    "pickle_out = open(\"X.pickle\", \"wb\")\n",
    "pickle.dump(X, pickle_out)\n",
    "pickle_out.close()\n",
    "\n",
    "pickle_out = open(\"y.pickle\", \"wb\")\n",
    "pickle.dump(y, pickle_out)\n",
    "pickle_out.close()\n",
    "\n",
    "pickle_in = open(\"X.pickle\", \"rb\")\n",
    "X = pickle.load(pickle_in)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 100, 3)\n",
      "(100, 100, 3)\n",
      "(100, 100, 3)\n",
      "(100, 100, 3)\n",
      "(100, 100, 3)\n",
      "(100, 100, 3)\n",
      "(100, 100, 3)\n",
      "(100, 100, 3)\n",
      "(100, 100, 3)\n",
      "(100, 100, 3)\n",
      "(100, 100, 3)\n",
      "(100, 100, 3)\n",
      "(100, 100, 3)\n",
      "(100, 100, 3)\n",
      "(100, 100, 3)\n",
      "(100, 100, 3)\n",
      "(100, 100, 3)\n",
      "(100, 100, 3)\n",
      "(100, 100, 3)\n",
      "(100, 100, 3)\n",
      "(100, 100, 3)\n",
      "(100, 100, 3)\n",
      "(100, 100, 3)\n",
      "(100, 100, 3)\n",
      "(100, 100, 3)\n",
      "(100, 100, 3)\n",
      "(100, 100, 3)\n",
      "(100, 100, 3)\n",
      "(100, 100, 3)\n",
      "(100, 100, 3)\n",
      "(100, 100, 3)\n",
      "(100, 100, 3)\n",
      "(100, 100, 3)\n",
      "(100, 100, 3)\n",
      "(100, 100, 3)\n",
      "(100, 100, 3)\n",
      "(100, 100, 3)\n",
      "(100, 100, 3)\n",
      "(100, 100, 3)\n",
      "(100, 100, 3)\n",
      "(100, 100, 3)\n",
      "(100, 100, 3)\n",
      "(100, 100, 3)\n",
      "(100, 100, 3)\n",
      "(100, 100, 3)\n",
      "(100, 100, 3)\n",
      "(100, 100, 3)\n",
      "(100, 100, 3)\n",
      "(100, 100, 3)\n",
      "(100, 100, 3)\n",
      "(100, 100, 3)\n",
      "(100, 100, 3)\n",
      "(100, 100, 3)\n",
      "(100, 100, 3)\n",
      "(100, 100, 3)\n",
      "(100, 100, 3)\n",
      "(100, 100, 3)\n",
      "(100, 100, 3)\n",
      "(100, 100, 3)\n",
      "(100, 100, 3)\n",
      "(100, 100, 3)\n",
      "(100, 100, 3)\n",
      "(100, 100, 3)\n",
      "(100, 100, 3)\n",
      "(100, 100, 3)\n",
      "(100, 100, 3)\n",
      "(100, 100, 3)\n",
      "(100, 100, 3)\n",
      "(100, 100, 3)\n",
      "(100, 100, 3)\n",
      "(100, 100, 3)\n",
      "(100, 100, 3)\n",
      "(100, 100, 3)\n",
      "(100, 100, 3)\n",
      "(100, 100, 3)\n",
      "(100, 100, 3)\n",
      "(100, 100, 3)\n",
      "(100, 100, 3)\n",
      "(100, 100, 3)\n",
      "(100, 100, 3)\n",
      "(100, 100, 3)\n",
      "(100, 100, 3)\n",
      "(100, 100, 3)\n",
      "(100, 100, 3)\n",
      "(100, 100, 3)\n",
      "(100, 100, 3)\n",
      "(100, 100, 3)\n",
      "(100, 100, 3)\n",
      "(100, 100, 3)\n",
      "(100, 100, 3)\n",
      "(100, 100, 3)\n",
      "(100, 100, 3)\n",
      "(100, 100, 3)\n",
      "(100, 100, 3)\n",
      "(100, 100, 3)\n",
      "(100, 100, 3)\n",
      "(100, 100, 3)\n",
      "(100, 100, 3)\n",
      "(100, 100, 3)\n",
      "(100, 100, 3)\n",
      "(100, 100, 3)\n",
      "(100, 100, 3)\n",
      "(100, 100, 3)\n",
      "(100, 100, 3)\n",
      "(100, 100, 3)\n",
      "(100, 100, 3)\n",
      "(100, 100, 3)\n",
      "(100, 100, 3)\n",
      "(100, 100, 3)\n",
      "(100, 100, 3)\n",
      "(100, 100, 3)\n",
      "(100, 100, 3)\n",
      "(100, 100, 3)\n",
      "(100, 100, 3)\n",
      "(100, 100, 3)\n",
      "(100, 100, 3)\n",
      "(100, 100, 3)\n",
      "(100, 100, 3)\n",
      "(100, 100, 3)\n",
      "(100, 100, 3)\n",
      "(100, 100, 3)\n",
      "(100, 100, 3)\n",
      "(100, 100, 3)\n",
      "(100, 100, 3)\n",
      "(100, 100, 3)\n",
      "(100, 100, 3)\n",
      "(100, 100, 3)\n",
      "(100, 100, 3)\n",
      "(100, 100, 3)\n",
      "(100, 100, 3)\n",
      "130\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "#import tensorflow.keras.models as Sequential\n",
    "#import tensorflow.keras.layers as Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D\n",
    "\n",
    "# from tensorflow.keras import models\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "#from tensorflow.keras.models import Sequential\n",
    "#from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n",
    "#from layers import Conv2D, MaxPooling2D\n",
    "\n",
    "# import pickle\n",
    "\n",
    "# X = pickle.load(open(\"X.pickle\", \"rb\"))\n",
    "# y = pickle.load(open(\"y.pickle\", \"rb\"))\n",
    "\n",
    "# X = X/255.0\n",
    "\n",
    "\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "main_path = \"/home/jovyan/Exam_Project/Dataset/\"\n",
    "data = os.listdir(main_path)\n",
    "#y_train skal bestå af en liste over hvert billedes kategori nummer\n",
    "#x_train skal bestå af 133 billeder, dvs. en liste med 130 lister i, hver bestående af nogle tuples\n",
    "\n",
    "# Hvert billede består af 784 tuples\n",
    "# Der skal random vælges 13 billeder fra hver kategori til train\n",
    "\n",
    "x_train_list = []\n",
    "y_train_list = []\n",
    "x_test_list = []\n",
    "y_test_list = []\n",
    "list_temp = []\n",
    "\n",
    "for d in data:\n",
    "    path = os.path.join(main_path, d)\n",
    "    # For hver path ligger der en række billeder, dem skal vi have proppet ind i en liste og valgt 13 ud\n",
    "    f = r''+path\n",
    "    \n",
    "    full_file_list = []\n",
    "    for file in os.listdir(f):\n",
    "        full_file_list.append(file)\n",
    "    \n",
    "    lsorted = sorted(full_file_list,key=lambda x: int(os.path.splitext(x)[0]))\n",
    "    \n",
    "    randomlist = []\n",
    "    randomlist = random.sample(range(0, 20), 13)\n",
    "    full_number_list = list(range(0,20))\n",
    "    x_test_list = full_number_list\n",
    "    \n",
    "    done_list = []\n",
    "    for ran in randomlist:\n",
    "        for fi in full_number_list:\n",
    "            if ran==fi:\n",
    "                x_test_list.remove(ran)\n",
    "        \n",
    "        for l in lsorted:\n",
    "            x = l.split(\".\")\n",
    "            if x[0] == str(ran):\n",
    "                done_list.append(l) # done_list indeholder 13 random billeder pr. kategori\n",
    "    \n",
    "    for r in x_test_list:\n",
    "        x_test_list[x_test_list.index(r)] = (str(r)+\".png\")\n",
    "        y_test_list.append(data.index(d))\n",
    "    \n",
    "    for done in done_list:\n",
    "        f_img = f+\"/\"+done\n",
    "        \n",
    "        i = Image.open(f_img)\n",
    "        \n",
    "        print(np.array(i).shape)\n",
    "        \n",
    "        list_temp.append(np.array(i))\n",
    "        y_train_list.append(data.index(d))\n",
    "print(len(list_temp))\n",
    "x_train = np.array(list_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "(130, 100, 100, 3)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "#x_train=np.array([np.array(c) for c in k for k in x_train])\n",
    "print(type(x_train))\n",
    "print(x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_train before: \n",
      "(130,)\n",
      "y_train after: \n",
      "(130, 10)\n",
      "[[[[0.3647059 ]\n",
      "   [0.41568628]\n",
      "   [0.35686275]\n",
      "   ...\n",
      "   [0.18431373]\n",
      "   [0.05098039]\n",
      "   [0.8156863 ]]\n",
      "\n",
      "  [[0.5529412 ]\n",
      "   [0.3254902 ]\n",
      "   [0.24705882]\n",
      "   ...\n",
      "   [0.5372549 ]\n",
      "   [0.05882353]\n",
      "   [0.11764706]]\n",
      "\n",
      "  [[0.32941177]\n",
      "   [0.6627451 ]\n",
      "   [0.13333334]\n",
      "   ...\n",
      "   [0.59607846]\n",
      "   [0.5921569 ]\n",
      "   [0.32941177]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.22745098]\n",
      "   [0.0627451 ]\n",
      "   [0.2901961 ]\n",
      "   ...\n",
      "   [0.14509805]\n",
      "   [0.14901961]\n",
      "   [0.07058824]]\n",
      "\n",
      "  [[0.32156864]\n",
      "   [0.04313726]\n",
      "   [0.28235295]\n",
      "   ...\n",
      "   [0.23137255]\n",
      "   [0.22745098]\n",
      "   [0.12156863]]\n",
      "\n",
      "  [[0.4117647 ]\n",
      "   [0.00784314]\n",
      "   [0.81960785]\n",
      "   ...\n",
      "   [0.3529412 ]\n",
      "   [0.39607844]\n",
      "   [0.30588236]]]\n",
      "\n",
      "\n",
      " [[[0.99607843]\n",
      "   [0.99607843]\n",
      "   [0.99607843]\n",
      "   ...\n",
      "   [0.99607843]\n",
      "   [0.99607843]\n",
      "   [0.99607843]]\n",
      "\n",
      "  [[0.99607843]\n",
      "   [0.99607843]\n",
      "   [0.99607843]\n",
      "   ...\n",
      "   [0.99607843]\n",
      "   [0.99607843]\n",
      "   [0.99607843]]\n",
      "\n",
      "  [[0.99607843]\n",
      "   [0.99607843]\n",
      "   [0.99607843]\n",
      "   ...\n",
      "   [0.99607843]\n",
      "   [0.99607843]\n",
      "   [0.99607843]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.99607843]\n",
      "   [0.99607843]\n",
      "   [0.99607843]\n",
      "   ...\n",
      "   [0.99215686]\n",
      "   [0.99215686]\n",
      "   [0.99215686]]\n",
      "\n",
      "  [[0.99607843]\n",
      "   [0.99607843]\n",
      "   [0.99607843]\n",
      "   ...\n",
      "   [0.99215686]\n",
      "   [0.99215686]\n",
      "   [0.99215686]]\n",
      "\n",
      "  [[0.99607843]\n",
      "   [0.99607843]\n",
      "   [0.99607843]\n",
      "   ...\n",
      "   [0.99607843]\n",
      "   [0.99607843]\n",
      "   [0.99607843]]]\n",
      "\n",
      "\n",
      " [[[1.        ]\n",
      "   [1.        ]\n",
      "   [1.        ]\n",
      "   ...\n",
      "   [0.9882353 ]\n",
      "   [0.9882353 ]\n",
      "   [0.99607843]]\n",
      "\n",
      "  [[1.        ]\n",
      "   [1.        ]\n",
      "   [1.        ]\n",
      "   ...\n",
      "   [0.99607843]\n",
      "   [0.9843137 ]\n",
      "   [0.99215686]]\n",
      "\n",
      "  [[1.        ]\n",
      "   [1.        ]\n",
      "   [1.        ]\n",
      "   ...\n",
      "   [0.99607843]\n",
      "   [0.9882353 ]\n",
      "   [0.99215686]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[1.        ]\n",
      "   [1.        ]\n",
      "   [1.        ]\n",
      "   ...\n",
      "   [1.        ]\n",
      "   [1.        ]\n",
      "   [1.        ]]\n",
      "\n",
      "  [[1.        ]\n",
      "   [1.        ]\n",
      "   [1.        ]\n",
      "   ...\n",
      "   [1.        ]\n",
      "   [1.        ]\n",
      "   [1.        ]]\n",
      "\n",
      "  [[1.        ]\n",
      "   [1.        ]\n",
      "   [1.        ]\n",
      "   ...\n",
      "   [1.        ]\n",
      "   [1.        ]\n",
      "   [1.        ]]]\n",
      "\n",
      "\n",
      " ...\n",
      "\n",
      "\n",
      " [[[1.        ]\n",
      "   [1.        ]\n",
      "   [1.        ]\n",
      "   ...\n",
      "   [1.        ]\n",
      "   [1.        ]\n",
      "   [1.        ]]\n",
      "\n",
      "  [[1.        ]\n",
      "   [1.        ]\n",
      "   [1.        ]\n",
      "   ...\n",
      "   [1.        ]\n",
      "   [1.        ]\n",
      "   [1.        ]]\n",
      "\n",
      "  [[1.        ]\n",
      "   [1.        ]\n",
      "   [1.        ]\n",
      "   ...\n",
      "   [1.        ]\n",
      "   [1.        ]\n",
      "   [1.        ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.99215686]\n",
      "   [0.99215686]\n",
      "   [0.9882353 ]\n",
      "   ...\n",
      "   [1.        ]\n",
      "   [1.        ]\n",
      "   [1.        ]]\n",
      "\n",
      "  [[0.99607843]\n",
      "   [0.99607843]\n",
      "   [0.99607843]\n",
      "   ...\n",
      "   [1.        ]\n",
      "   [1.        ]\n",
      "   [1.        ]]\n",
      "\n",
      "  [[1.        ]\n",
      "   [1.        ]\n",
      "   [1.        ]\n",
      "   ...\n",
      "   [0.99607843]\n",
      "   [0.99607843]\n",
      "   [1.        ]]]\n",
      "\n",
      "\n",
      " [[[0.9882353 ]\n",
      "   [0.99607843]\n",
      "   [0.99215686]\n",
      "   ...\n",
      "   [0.99607843]\n",
      "   [0.99607843]\n",
      "   [0.99607843]]\n",
      "\n",
      "  [[0.99607843]\n",
      "   [0.99607843]\n",
      "   [0.99607843]\n",
      "   ...\n",
      "   [1.        ]\n",
      "   [1.        ]\n",
      "   [1.        ]]\n",
      "\n",
      "  [[0.99607843]\n",
      "   [0.99607843]\n",
      "   [0.99215686]\n",
      "   ...\n",
      "   [0.99607843]\n",
      "   [1.        ]\n",
      "   [1.        ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[1.        ]\n",
      "   [0.9882353 ]\n",
      "   [1.        ]\n",
      "   ...\n",
      "   [0.9843137 ]\n",
      "   [0.9843137 ]\n",
      "   [0.9882353 ]]\n",
      "\n",
      "  [[0.9882353 ]\n",
      "   [1.        ]\n",
      "   [0.99215686]\n",
      "   ...\n",
      "   [0.9843137 ]\n",
      "   [0.98039216]\n",
      "   [0.9882353 ]]\n",
      "\n",
      "  [[1.        ]\n",
      "   [1.        ]\n",
      "   [1.        ]\n",
      "   ...\n",
      "   [0.99215686]\n",
      "   [0.99607843]\n",
      "   [0.99607843]]]\n",
      "\n",
      "\n",
      " [[[1.        ]\n",
      "   [1.        ]\n",
      "   [1.        ]\n",
      "   ...\n",
      "   [1.        ]\n",
      "   [1.        ]\n",
      "   [1.        ]]\n",
      "\n",
      "  [[1.        ]\n",
      "   [1.        ]\n",
      "   [1.        ]\n",
      "   ...\n",
      "   [1.        ]\n",
      "   [1.        ]\n",
      "   [1.        ]]\n",
      "\n",
      "  [[1.        ]\n",
      "   [1.        ]\n",
      "   [1.        ]\n",
      "   ...\n",
      "   [1.        ]\n",
      "   [1.        ]\n",
      "   [1.        ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[1.        ]\n",
      "   [1.        ]\n",
      "   [1.        ]\n",
      "   ...\n",
      "   [1.        ]\n",
      "   [1.        ]\n",
      "   [1.        ]]\n",
      "\n",
      "  [[1.        ]\n",
      "   [1.        ]\n",
      "   [1.        ]\n",
      "   ...\n",
      "   [1.        ]\n",
      "   [1.        ]\n",
      "   [1.        ]]\n",
      "\n",
      "  [[1.        ]\n",
      "   [1.        ]\n",
      "   [1.        ]\n",
      "   ...\n",
      "   [1.        ]\n",
      "   [1.        ]\n",
      "   [1.        ]]]]\n",
      "Model: \"sequential_29\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_74 (Conv2D)           (None, 48, 48, 32)        320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_54 (MaxPooling (None, 24, 24, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_75 (Conv2D)           (None, 22, 22, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_55 (MaxPooling (None, 11, 11, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_76 (Conv2D)           (None, 9, 9, 64)          36928     \n",
      "_________________________________________________________________\n",
      "flatten_27 (Flatten)         (None, 5184)              0         \n",
      "_________________________________________________________________\n",
      "dense_54 (Dense)             (None, 64)                331840    \n",
      "_________________________________________________________________\n",
      "dense_55 (Dense)             (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 388,234\n",
      "Trainable params: 388,234\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import to_categorical\n",
    "num_classes = 10\n",
    "y_train = np.array(y_train_list)\n",
    "x_test = np.array(x_test_list)\n",
    "y_test = np.array(y_test_list)\n",
    "print(\"y_train before: \")\n",
    "print(y_train.shape)\n",
    "training_data = (x_train, y_train)\n",
    "test_data = (x_test, y_test)\n",
    "\n",
    "# ()\n",
    "\n",
    "# print(x_train.shape)\n",
    "# x_train = x_train.astype('float32')\n",
    "# x_train /= 255\n",
    "\n",
    "\n",
    "# x_test = x_test.astype('float32')\n",
    "# x_test /= 255\n",
    "\n",
    "#reformat output\n",
    "y_train = to_categorical(y_train, num_classes)\n",
    "print(\"y_train after: \")\n",
    "print(y_train.shape)\n",
    "\n",
    "y_test = to_categorical(y_test, num_classes)\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "print(X)\n",
    "model.add(layers.Conv2D(32, (3,3), activation='relu', input_shape=X.shape[1:]))\n",
    "model.add(layers.MaxPool2D((2,2))) # Max Pooling to reduce the spatial dimensions of the output volume. pool_size: integer or tuple of 2 integers, factors by which to downscale (vertical, horizontal)\n",
    "model.add(layers.Conv2D(64,(3,3),activation='relu')) # does not need input_shape, since it gets it from previous layer\n",
    "model.add(layers.MaxPool2D((2,2)))\n",
    "model.add(layers.Conv2D(64,(3,3),activation='relu'))\n",
    "model.add(layers.Flatten()) # rewrite tensor to single vector of values\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(10, activation='softmax')) # softmax is good for output layer because Softmax outputs probabilities range. The range will 0 to 1, and the sum of all the probabilities will be equal to one. If the softmax function used for multi-classification model it returns the probabilities of each class and the target class will have the high probability.\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    /opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py:805 train_function  *\n        return step_function(self, iterator)\n    /opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py:795 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    /opt/conda/lib/python3.7/site-packages/tensorflow/python/distribute/distribute_lib.py:1259 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    /opt/conda/lib/python3.7/site-packages/tensorflow/python/distribute/distribute_lib.py:2730 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    /opt/conda/lib/python3.7/site-packages/tensorflow/python/distribute/distribute_lib.py:3417 _call_for_each_replica\n        return fn(*args, **kwargs)\n    /opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py:788 run_step  **\n        outputs = model.train_step(data)\n    /opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py:754 train_step\n        y_pred = self(x, training=True)\n    /opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py:998 __call__\n        input_spec.assert_input_compatibility(self.input_spec, inputs, self.name)\n    /opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/input_spec.py:259 assert_input_compatibility\n        ' but received input with shape ' + display_shape(x.shape))\n\n    ValueError: Input 0 of layer sequential_28 is incompatible with the layer: expected axis -1 of input shape to have value 1 but received input with shape (None, 100, 100, 3)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-336-32eb90bb41b2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m          \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m          \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m          batch_size=64) # checking periodically how well we are doing\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1098\u001b[0m                 _r=1):\n\u001b[1;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    869\u001b[0m       \u001b[0;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    870\u001b[0m       \u001b[0minitializers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 871\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    872\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    873\u001b[0m       \u001b[0;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[0;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[1;32m    724\u001b[0m     self._concrete_stateful_fn = (\n\u001b[1;32m    725\u001b[0m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[0;32m--> 726\u001b[0;31m             *args, **kwds))\n\u001b[0m\u001b[1;32m    727\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    728\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minvalid_creator_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0munused_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0munused_kwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2967\u001b[0m       \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2968\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2969\u001b[0;31m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2970\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2971\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   3359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3360\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3361\u001b[0;31m           \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3362\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   3204\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3205\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3206\u001b[0;31m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[1;32m   3207\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3208\u001b[0m         \u001b[0mfunction_spec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_spec\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m    988\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    989\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 990\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    991\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    992\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    632\u001b[0m             \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 634\u001b[0;31m           \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    635\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    975\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    976\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 977\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    978\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    979\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    /opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py:805 train_function  *\n        return step_function(self, iterator)\n    /opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py:795 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    /opt/conda/lib/python3.7/site-packages/tensorflow/python/distribute/distribute_lib.py:1259 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    /opt/conda/lib/python3.7/site-packages/tensorflow/python/distribute/distribute_lib.py:2730 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    /opt/conda/lib/python3.7/site-packages/tensorflow/python/distribute/distribute_lib.py:3417 _call_for_each_replica\n        return fn(*args, **kwargs)\n    /opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py:788 run_step  **\n        outputs = model.train_step(data)\n    /opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py:754 train_step\n        y_pred = self(x, training=True)\n    /opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py:998 __call__\n        input_spec.assert_input_compatibility(self.input_spec, inputs, self.name)\n    /opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/input_spec.py:259 assert_input_compatibility\n        ' but received input with shape ' + display_shape(x.shape))\n\n    ValueError: Input 0 of layer sequential_28 is incompatible with the layer: expected axis -1 of input shape to have value 1 but received input with shape (None, 100, 100, 3)\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy', # loss is how to meassure how wrong the model is on its predictions\n",
    "             optimizer='adam', # \"stochastic gradient descent\" is a way to tell algorithm how to improve\n",
    "             metrics=['accuracy'], # what do we care about in our model\n",
    "             )\n",
    "print(type(x_train))\n",
    "print(type(y_train))\n",
    "model.fit(x_train,\n",
    "         y_train,\n",
    "         epochs=5,\n",
    "         verbose=True,\n",
    "         validation_split=0.1,\n",
    "         batch_size=64) # checking periodically how well we are doing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_16 (Conv2D)           (None, 48, 48, 256)       2560      \n",
      "_________________________________________________________________\n",
      "activation_24 (Activation)   (None, 48, 48, 256)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_16 (MaxPooling (None, 24, 24, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_17 (Conv2D)           (None, 22, 22, 256)       590080    \n",
      "_________________________________________________________________\n",
      "activation_25 (Activation)   (None, 22, 22, 256)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_17 (MaxPooling (None, 11, 11, 256)       0         \n",
      "_________________________________________________________________\n",
      "flatten_8 (Flatten)          (None, 30976)             0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 64)                1982528   \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 1)                 65        \n",
      "_________________________________________________________________\n",
      "activation_26 (Activation)   (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 2,575,233\n",
      "Trainable params: 2,575,233\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dropout, Activation, Flatten\n",
    "\n",
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "X = pd.read_pickle(r'/home/jovyan/Exam_Project/X.pickle')\n",
    "X = X.astype('float32')\n",
    "y = pd.read_pickle(r'/home/jovyan/Exam_Project/y.pickle')\n",
    "\n",
    "#X = X.astype('uint8')\n",
    "#X = pickle.load(open(\"X.pickle\", \"rb\"))\n",
    "#y = pickle.load(open(\"y.pickle\", \"rb\"))\n",
    "\n",
    "X = X/255.0\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(256, (3, 3), input_shape=X.shape[1:]))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(256, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Flatten())  # this converts our 3D feature maps to 1D feature vectors\n",
    "\n",
    "model.add(Dense(64))\n",
    "\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "<class 'list'>\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "`validation_split` is only supported for Tensors or NumPy arrays, found following types in the input: [<class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-eafce35720f2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m          \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m          \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m          validation_split=0.1) # checking periodically how well we are doing\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1039\u001b[0m       (x, y, sample_weight), validation_data = (\n\u001b[1;32m   1040\u001b[0m           data_adapter.train_validation_split(\n\u001b[0;32m-> 1041\u001b[0;31m               (x, y, sample_weight), validation_split=validation_split))\n\u001b[0m\u001b[1;32m   1042\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1043\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36mtrain_validation_split\u001b[0;34m(arrays, validation_split)\u001b[0m\n\u001b[1;32m   1357\u001b[0m     raise ValueError(\n\u001b[1;32m   1358\u001b[0m         \u001b[0;34m\"`validation_split` is only supported for Tensors or NumPy \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1359\u001b[0;31m         \"arrays, found following types in the input: {}\".format(unsplitable))\n\u001b[0m\u001b[1;32m   1360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1361\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mflat_arrays\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: `validation_split` is only supported for Tensors or NumPy arrays, found following types in the input: [<class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>]"
     ]
    }
   ],
   "source": [
    "# model.compile(loss='binary_crossentropy',\n",
    "#               optimizer='adam',\n",
    "#               metrics=['accuracy'])\n",
    "\n",
    "# print(type(X))\n",
    "\n",
    "\n",
    "# model.fit(X, y, batch_size=32, epochs=3, validation_split=0.3)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', # loss is how to meassure how wrong the model is on its predictions\n",
    "             optimizer='rmsprop', # \"stochastic gradient descent\" is a way to tell algorithm how to improve\n",
    "             metrics=['accuracy'], # what do we care about in our model\n",
    "             )\n",
    "print(type(X))\n",
    "print(type(y))\n",
    "\n",
    "model.fit(X,\n",
    "         y,\n",
    "         epochs=5,\n",
    "         verbose=True,\n",
    "         batch_size=64,\n",
    "         validation_split=0.1) # checking periodically how well we are doing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0.png',\n",
       " '1.png',\n",
       " '10.png',\n",
       " '11.png',\n",
       " '12.png',\n",
       " '13.png',\n",
       " '14.png',\n",
       " '15.png',\n",
       " '16.png',\n",
       " '17.png',\n",
       " '18.png',\n",
       " '19.png',\n",
       " '2.png',\n",
       " '3.png',\n",
       " '4.png',\n",
       " '5.png',\n",
       " '6.png',\n",
       " '7.png',\n",
       " '8.png',\n",
       " '9.png']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "data = os.listdir('/home/jovyan/Exam_Project/Dataset/Ananas')\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TEST:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "import cv2\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "file_list = []\n",
    "class_list = []\n",
    "\n",
    "DATADIR = \"/home/jovyan/Exam_Project/Dataset\"\n",
    "\n",
    "# All the categories you want your neural network to detect\n",
    "CATEGORIES = [\"Banan\", \"Æble\", \"Appelsin\", \"Pære\", \"Kiwi\", \"Citron\", \"Vandmelon\", \"Ananas\", \"Vindruer\", \"Mango\"]\n",
    "\n",
    "# The size of the images that your neural network will use\n",
    "IMG_SIZE = 50\n",
    "\n",
    "# Checking or all images in the data folder\n",
    "for category in CATEGORIES :\n",
    "    path = os.path.join(DATADIR, category)\n",
    "    for img in os.listdir(path):\n",
    "        img_array = cv2.imread(os.path.join(path, img), cv2.IMREAD_GRAYSCALE)\n",
    "        \n",
    "\n",
    "training_data = []\n",
    "\n",
    "\n",
    "def create_training_data():\n",
    "    print(\"Creating training data...\")\n",
    "    for category in CATEGORIES :\n",
    "        path = os.path.join(DATADIR, category)\n",
    "        class_num = CATEGORIES.index(category)\n",
    "        for img in os.listdir(path):\n",
    "            try :\n",
    "                img_array = cv2.imread(os.path.join(path, img), cv2.IMREAD_GRAYSCALE)\n",
    "                new_array = cv2.resize(img_array, (IMG_SIZE, IMG_SIZE))\n",
    "                training_data.append([new_array, class_num])\n",
    "            except Exception as e:\n",
    "                pass\n",
    "            \n",
    "            \n",
    "create_training_data()\n",
    "print(len(training_data))\n",
    "random.shuffle(training_data)\n",
    "\n",
    "\n",
    "X = [] #features\n",
    "y = [] #labels\n",
    "\n",
    "for features, label in training_data:\n",
    "    X.append(features)\n",
    "    y.append(label)\n",
    "    \n",
    "X = np.array(X).reshape(-1, IMG_SIZE, IMG_SIZE, 1)\n",
    "\n",
    "# Creating the files containing all the information about your model\n",
    "pickle_out = open(\"X.pickle\", \"wb\")\n",
    "pickle.dump(X, pickle_out)\n",
    "pickle_out.close()\n",
    "\n",
    "pickle_out = open(\"y.pickle\", \"wb\")\n",
    "pickle.dump(y, pickle_out)\n",
    "pickle_out.close()\n",
    "\n",
    "pickle_in = open(\"X.pickle\", \"rb\")\n",
    "X = pickle.load(pickle_in)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.metrics import accuracy_score \n",
    "\n",
    "dataset = create_dataset()\n",
    "\n",
    "print(type(dataset))\n",
    "print(dataset.keys())\n",
    "X = dataset.data # features \n",
    "y = dataset.target # result or classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
